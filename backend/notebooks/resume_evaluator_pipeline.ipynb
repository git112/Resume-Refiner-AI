{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60b8d605",
   "metadata": {},
   "source": [
    "PHASE 1: CLEANING & PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c21ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load datasets\n",
    "job_df = pd.read_csv('IT_Job_Roles_Skills.csv', encoding='latin1')\n",
    "resume_df = pd.read_csv('UpdatedResumeDataSet.csv', encoding='latin1')\n",
    "\n",
    "# Explore\n",
    "print(job_df.info())\n",
    "print(job_df.head())\n",
    "\n",
    "print(resume_df.info())\n",
    "print(resume_df.head())\n",
    "\n",
    "# Check missing values\n",
    "print(job_df.isnull().sum())\n",
    "print(resume_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f01c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_skills(skill_str):\n",
    "    if pd.isna(skill_str):\n",
    "        return []\n",
    "    return [skill.strip().lower() for skill in skill_str.split(',')]\n",
    "\n",
    "job_df['Skills_clean'] = job_df['Skills'].apply(clean_skills)\n",
    "\n",
    "# Similarly clean Job Titles and Descriptions\n",
    "job_df['Job Title'] = job_df['Job Title'].str.strip().str.lower()\n",
    "job_df['Job Description'] = job_df['Job Description'].fillna('').str.lower()\n",
    "\n",
    "resume_df = resume_df.dropna(subset=['Resume'])  # Assuming 'Resume' column name\n",
    "resume_df['Resume'] = resume_df['Resume'].str.lower()\n",
    "\n",
    "\n",
    "if 'Skills' in resume_df.columns:\n",
    "    resume_df['Skills_clean'] = resume_df['Skills'].apply(clean_skills)\n",
    "else:\n",
    "    \n",
    "    pass\n",
    "\n",
    "# Summary stats\n",
    "print(\"Unique skills in jobs:\", set(sum(job_df['Skills_clean'], [])))\n",
    "print(\"Unique skills in resumes:\", set(sum(resume_df['Skills_clean'], [])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13ae393",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "job_df['Job Title'] = job_df['Job Title'].str.strip().str.lower()\n",
    "job_df['Job Description'] = job_df['Job Description'].fillna('')\n",
    "\n",
    "all_job_skills = set(skill for skills_list in job_df['Skills_clean'] for skill in skills_list)\n",
    "\n",
    "def extract_skills_from_resume(resume_text):\n",
    "    if pd.isna(resume_text):\n",
    "        return []\n",
    "    \n",
    "    resume_text = resume_text.lower()\n",
    "    found_skills = []\n",
    "    \n",
    "    for skill in all_job_skills:\n",
    "        if skill in resume_text:\n",
    "            found_skills.append(skill)\n",
    "    \n",
    "    return found_skills\n",
    "\n",
    "resume_df['Skills_extracted'] = resume_df['Resume'].apply(extract_skills_from_resume)\n",
    "\n",
    "resume_df['Skills_count'] = resume_df['Skills_extracted'].apply(len)\n",
    "\n",
    "resume_df['Top_skill_areas'] = resume_df['Skills_extracted'].apply(\n",
    "    lambda skills: ', '.join(skills[:5]) if len(skills) > 0 else \"No matching skills found\"\n",
    ")\n",
    "\n",
    "print(f\"Jobs dataset shape: {job_df.shape}\")\n",
    "print(f\"Resume dataset shape: {resume_df.shape}\")\n",
    "print(\"\\nSample of extracted skills from resumes:\")\n",
    "print(resume_df[['Category', 'Skills_extracted', 'Skills_count']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feee6a1e",
   "metadata": {},
   "source": [
    "PHASE 2: FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2cf258",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import scipy.sparse as sparse\n",
    "\n",
    "skill_vocabulary = sorted(list(all_job_skills))\n",
    "print(f\"Total unique skills in vocabulary: {len(skill_vocabulary)}\")\n",
    "\n",
    "def create_skill_vector(skills_list):\n",
    "\n",
    "    indices = [skill_vocabulary.index(skill) for skill in skills_list if skill in skill_vocabulary]\n",
    "    if not indices:\n",
    "        return sparse.csr_matrix((1, len(skill_vocabulary)), dtype=np.int8)\n",
    "    \n",
    "    data = np.ones(len(indices), dtype=np.int8)\n",
    "    indptr = np.array([0, len(indices)])\n",
    "    return sparse.csr_matrix((data, indices, indptr), shape=(1, len(skill_vocabulary)))\n",
    "\n",
    "# Apply vectorization\n",
    "job_skill_vectors = sparse.vstack([create_skill_vector(skills) for skills in job_df['Skills_clean']])\n",
    "resume_skill_vectors = sparse.vstack([create_skill_vector(skills) for skills in resume_df['Skills_extracted']])\n",
    "\n",
    "print(f\"Job skill vectors shape: {job_skill_vectors.shape}\")\n",
    "print(f\"Resume skill vectors shape: {resume_skill_vectors.shape}\")\n",
    "\n",
    "if 'Category' in job_df.columns:\n",
    "    job_encoder = OneHotEncoder(sparse_output=True)\n",
    "    job_categories = job_encoder.fit_transform(job_df[['Category']])\n",
    "    print(f\"Job categories encoded shape: {job_categories.shape}\")\n",
    "\n",
    "resume_encoder = OneHotEncoder(sparse_output=True)\n",
    "resume_categories = resume_encoder.fit_transform(resume_df[['Category']])\n",
    "print(f\"Resume categories encoded shape: {resume_categories.shape}\")\n",
    "\n",
    "print(\"Creating semantic embeddings with Sentence Transformer...\")\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "sample_size = min(50, len(job_df))\n",
    "sample_jobs = job_df.sample(sample_size)\n",
    "sample_resumes = resume_df.sample(sample_size)\n",
    "s\n",
    "sample_job_embeddings = model.encode(sample_jobs['Job Description'].tolist(), \n",
    "                                    show_progress_bar=True,\n",
    "                                    batch_size=16)\n",
    "sample_resume_embeddings = model.encode(sample_resumes['Resume'].tolist(),\n",
    "                                       show_progress_bar=True,\n",
    "                                       batch_size=16)\n",
    "\n",
    "print(f\"Job embedding dimensions: {sample_job_embeddings.shape}\")\n",
    "print(f\"Resume embedding dimensions: {sample_resume_embeddings.shape}\")\n",
    "\n",
    "resume_df['Text_Length'] = resume_df['Resume'].str.len()\n",
    "resume_df['Word_Count'] = resume_df['Resume'].apply(lambda x: len(str(x).split()))\n",
    "resume_df['Skills_Density'] = resume_df['Skills_count'] / resume_df['Word_Count'].clip(lower=1)\n",
    "\n",
    "print(\"\\nFeature engineering complete!\")\n",
    "print(\"Features created:\")\n",
    "print(\"1. Sparse skill vectors for efficient representation\")\n",
    "print(\"2. Category encodings for both resumes and jobs\")\n",
    "print(\"3. Semantic embeddings using Sentence Transformer\")\n",
    "print(\"4. Text statistics (length, word count, skills density)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22edb30d",
   "metadata": {},
   "source": [
    "PHASE 3: MODEL TRAINING FOR JOB-RESUME MATCHING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941a6249",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import scipy.sparse as sp\n",
    "# Create pairs of resume-job data with similarity metrics\n",
    "\n",
    "n_resumes = len(resume_df)\n",
    "n_jobs = len(job_df)\n",
    "\n",
    "max_pairs = 10000\n",
    "sample_ratio = min(1.0, max_pairs / (n_resumes * n_jobs))\n",
    "\n",
    "pairs = []\n",
    "\n",
    "resume_indices = list(range(n_resumes))\n",
    "job_indices = list(range(n_jobs))\n",
    "\n",
    "selected_pairs = []\n",
    "if sample_ratio < 1.0:\n",
    "   \n",
    "    for _ in range(max_pairs):\n",
    "        resume_idx = random.choice(resume_indices)\n",
    "        job_idx = random.choice(job_indices)\n",
    "        selected_pairs.append((resume_idx, job_idx))\n",
    "else:\n",
    "   \n",
    "    selected_pairs = [(i, j) for i in resume_indices[:100] for j in job_indices[:100]]\n",
    "\n",
    "for resume_idx, job_idx in selected_pairs:\n",
    "    resume_skills = set(resume_df.iloc[resume_idx]['Skills_extracted'])\n",
    "    job_skills = set(job_df.iloc[job_idx]['Skills_clean'])\n",
    "    \n",
    "    if len(resume_skills) == 0 or len(job_skills) == 0:\n",
    "        jaccard_similarity = 0.0\n",
    "    else:\n",
    "        jaccard_similarity = len(resume_skills.intersection(job_skills)) / len(resume_skills.union(job_skills))\n",
    "\n",
    "    common_skills_count = len(resume_skills.intersection(job_skills))\n",
    "    \n",
    "    resume_vec = resume_skill_vectors[resume_idx]\n",
    "    job_vec = job_skill_vectors[job_idx]\n",
    "    \n",
    "    skill_vector_similarity = 0.0\n",
    "    if not (resume_vec.nnz == 0 or job_vec.nnz == 0):\n",
    "        skill_vector_similarity = cosine_similarity(resume_vec, job_vec)[0][0]\n",
    "    \n",
    "    content_similarity = 0.0\n",
    "    \n",
    "    match_score = (0.3 * jaccard_similarity + \n",
    "                  0.2 * (common_skills_count / max(1, len(job_skills))) + \n",
    "                  0.5 * skill_vector_similarity)\n",
    "    \n",
    "    pair_data = {\n",
    "        'resume_idx': resume_idx,\n",
    "        'job_idx': job_idx,\n",
    "        'resume_category': resume_df.iloc[resume_idx]['Category'],\n",
    "        'job_title': job_df.iloc[job_idx]['Job Title'],\n",
    "        'jaccard_similarity': jaccard_similarity,\n",
    "        'common_skills_count': common_skills_count,\n",
    "        'content_similarity': content_similarity,\n",
    "        'skill_vector_similarity': skill_vector_similarity,\n",
    "        'match_score': match_score,\n",
    "        'skills_density': resume_df.iloc[resume_idx]['Skills_Density'],\n",
    "        'resume_skills_count': len(resume_skills),\n",
    "        'job_skills_count': len(job_skills)\n",
    "    }\n",
    "    \n",
    "    pairs.append(pair_data)\n",
    "\n",
    "# Create DataFrame with all pairs\n",
    "pairs_df = pd.DataFrame(pairs)\n",
    "\n",
    "def categorize_match(score):\n",
    "    if score == 0:\n",
    "        return float('nan')  \n",
    "    elif score < 0.1:\n",
    "        return \"Very Low\"\n",
    "    elif score < 0.25:\n",
    "        return \"Low\"\n",
    "    elif score < 0.5:\n",
    "        return \"Medium\"\n",
    "    elif score < 0.75:\n",
    "        return \"High\"\n",
    "    else:\n",
    "        return \"Very High\"\n",
    "\n",
    "pairs_df['match_level'] = pairs_df['match_score'].apply(categorize_match)\n",
    "pairs_df['match_level'] = pairs_df['match_level'].astype('category')\n",
    "\n",
    "pairs_df_model = pairs_df[['resume_idx', 'job_idx', 'jaccard_similarity', \n",
    "                          'common_skills_count', 'content_similarity', \n",
    "                          'skill_vector_similarity', 'match_score']]\n",
    "\n",
    "print(f\"Created {len(pairs_df)} resume-job pairs for analysis\")\n",
    "print(f\"Match level distribution:\\n{pairs_df['match_level'].value_counts(dropna=False)}\")\n",
    "print(f\"Match score statistics:\\n{pairs_df['match_score'].describe()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda362d5",
   "metadata": {},
   "source": [
    "PHASE 4: MODEL TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f13b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Preparing features for model training...\")\n",
    "\n",
    "model_data = pairs_df.copy()\n",
    "\n",
    "model_data['skill_count_diff'] = abs(model_data['resume_skills_count'] - model_data['job_skills_count'])\n",
    "model_data['skill_count_ratio'] = model_data['common_skills_count'] / model_data['job_skills_count'].clip(lower=1)\n",
    "\n",
    "\n",
    "features = [\n",
    "    'jaccard_similarity',\n",
    "    'common_skills_count', \n",
    "    'skill_vector_similarity',\n",
    "    'skill_count_diff',\n",
    "    'skill_count_ratio',\n",
    "    'skills_density'\n",
    "]\n",
    "\n",
    "model_data = model_data.dropna(subset=features)\n",
    "\n",
    "X = model_data[features].values\n",
    "y = model_data['match_score'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Training models with hyperparameter tuning...\")\n",
    "\n",
    "rf_params = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5]\n",
    "}\n",
    "\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "rf_grid = GridSearchCV(rf, rf_params, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "rf_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "gb_params = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'max_depth': [3, 5]\n",
    "}\n",
    "\n",
    "gb = GradientBoostingRegressor(random_state=42)\n",
    "gb_grid = GridSearchCV(gb, gb_params, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "gb_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "rf_best = rf_grid.best_estimator_\n",
    "gb_best = gb_grid.best_estimator_\n",
    "\n",
    "rf_pred = rf_best.predict(X_test_scaled)\n",
    "gb_pred = gb_best.predict(X_test_scaled)\n",
    "\n",
    "rf_rmse = np.sqrt(mean_squared_error(y_test, rf_pred))\n",
    "gb_rmse = np.sqrt(mean_squared_error(y_test, gb_pred))\n",
    "\n",
    "print(f\"Random Forest RMSE: {rf_rmse:.4f}\")\n",
    "print(f\"Gradient Boosting RMSE: {gb_rmse:.4f}\")\n",
    "\n",
    "if rf_rmse <= gb_rmse:\n",
    "    best_model = rf_best\n",
    "    best_pred = rf_pred\n",
    "    print(\"Random Forest selected as best model\")\n",
    "else:\n",
    "    best_model = gb_best\n",
    "    best_pred = gb_pred\n",
    "    print(\"Gradient Boosting selected as best model\")\n",
    "\n",
    "#Feature importance analysis\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Importance': best_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importance:\")\n",
    "print(feature_importance)\n",
    "\n",
    "#Evaluation metrics\n",
    "print(\"\\nModel Evaluation:\")\n",
    "print(f\"RMSE: {np.sqrt(mean_squared_error(y_test, best_pred)):.4f}\")\n",
    "print(f\"MAE: {mean_absolute_error(y_test, best_pred):.4f}\")\n",
    "print(f\"R² Score: {r2_score(y_test, best_pred):.4f}\")\n",
    "\n",
    "#Visualize predictions vs actual values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, best_pred, alpha=0.5)\n",
    "plt.plot([0, 1], [0, 1], 'r--')\n",
    "plt.xlabel('Actual Match Score')\n",
    "plt.ylabel('Predicted Match Score')\n",
    "plt.title('Predicted vs Actual Match Scores')\n",
    "plt.tight_layout()\n",
    "plt.savefig('match_score_prediction.png')\n",
    "plt.show()\n",
    "\n",
    "#Save the model for future use\n",
    "joblib.dump(best_model, 'resume_job_matching_model.pkl')\n",
    "joblib.dump(scaler, 'feature_scaler.pkl')\n",
    "joblib.dump(features, 'model_features.pkl')\n",
    "\n",
    "print(\"\\nModel saved as 'resume_job_matching_model.pkl'\")\n",
    "print(\"Feature scaler saved as 'feature_scaler.pkl'\")\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d349f313",
   "metadata": {},
   "source": [
    "PHASE 5: TESTING MODEL PERFORMANCE AND VISUALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4516f5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model = joblib.load('resume_job_matching_model.pkl')\n",
    "scaler = joblib.load('feature_scaler.pkl')\n",
    "features = joblib.load('model_features.pkl')\n",
    "\n",
    "def predict_match_score(resume_idx, job_idx):\n",
    "    \"\"\"Predict match score between a resume and job\"\"\"\n",
    "\n",
    "    resume_skills = set(resume_df.iloc[resume_idx]['Skills_extracted'])\n",
    "\n",
    "    job_skills = set(job_df.iloc[job_idx]['Skills_clean'])\n",
    "\n",
    "    jaccard_similarity = 0.0\n",
    "    if len(resume_skills) > 0 and len(job_skills) > 0:\n",
    "        jaccard_similarity = len(resume_skills.intersection(job_skills)) / len(resume_skills.union(job_skills))\n",
    "    \n",
    "    common_skills_count = len(resume_skills.intersection(job_skills))\n",
    "\n",
    "    resume_vec = resume_skill_vectors[resume_idx]\n",
    "    job_vec = job_skill_vectors[job_idx]\n",
    "    \n",
    "    skill_vector_similarity = 0.0\n",
    "    if not (resume_vec.nnz == 0 or job_vec.nnz == 0):\n",
    "        skill_vector_similarity = cosine_similarity(resume_vec, job_vec)[0][0]\n",
    "\n",
    "    skill_count_diff = abs(len(resume_skills) - len(job_skills))\n",
    "    skill_count_ratio = common_skills_count / max(1, len(job_skills))\n",
    "    skills_density = resume_df.iloc[resume_idx]['Skills_Density']\n",
    "    \n",
    "    X = np.array([[\n",
    "        jaccard_similarity,\n",
    "        common_skills_count,\n",
    "        skill_vector_similarity,\n",
    "        skill_count_diff,\n",
    "        skill_count_ratio,\n",
    "        skills_density\n",
    "    ]])\n",
    "\n",
    "    X_scaled = scaler.transform(X)\n",
    "\n",
    "    match_score = model.predict(X_scaled)[0]\n",
    "    \n",
    "    return {\n",
    "        'resume_category': resume_df.iloc[resume_idx]['Category'],\n",
    "        'job_title': job_df.iloc[job_idx]['Job Title'],\n",
    "        'match_score': match_score,\n",
    "        'common_skills': list(resume_skills.intersection(job_skills)),\n",
    "        'jaccard_similarity': jaccard_similarity,\n",
    "        'skill_vector_similarity': skill_vector_similarity\n",
    "    }\n",
    "\n",
    "np.random.seed(42)\n",
    "test_size = 100\n",
    "test_pairs = []\n",
    "\n",
    "for _ in range(test_size):\n",
    "    resume_idx = np.random.randint(0, len(resume_df))\n",
    "    job_idx = np.random.randint(0, len(job_df))\n",
    "\n",
    "    resume_skills = set(resume_df.iloc[resume_idx]['Skills_extracted'])\n",
    "    job_skills = set(job_df.iloc[job_idx]['Skills_clean'])\n",
    "    \n",
    "    jaccard_similarity = 0.0\n",
    "    if len(resume_skills) > 0 and len(job_skills) > 0:\n",
    "        jaccard_similarity = len(resume_skills.intersection(job_skills)) / len(resume_skills.union(job_skills))\n",
    "    \n",
    "    common_skills_count = len(resume_skills.intersection(job_skills))\n",
    "    \n",
    "    resume_vec = resume_skill_vectors[resume_idx]\n",
    "    job_vec = job_skill_vectors[job_idx]\n",
    "    \n",
    "    skill_vector_similarity = 0.0\n",
    "    if not (resume_vec.nnz == 0 or job_vec.nnz == 0):\n",
    "        skill_vector_similarity = cosine_similarity(resume_vec, job_vec)[0][0]\n",
    "    \n",
    "    actual_score = (0.3 * jaccard_similarity + \n",
    "                   0.2 * (common_skills_count / max(1, len(job_skills))) + \n",
    "                   0.5 * skill_vector_similarity)\n",
    "   \n",
    "    prediction = predict_match_score(resume_idx, job_idx)\n",
    "    \n",
    "    test_pairs.append({\n",
    "        'resume_idx': resume_idx,\n",
    "        'job_idx': job_idx,\n",
    "        'resume_category': resume_df.iloc[resume_idx]['Category'],\n",
    "        'job_title': job_df.iloc[job_idx]['Job Title'],\n",
    "        'actual_score': actual_score,\n",
    "        'predicted_score': prediction['match_score'],\n",
    "        'error': prediction['match_score'] - actual_score\n",
    "    })\n",
    "\n",
    "test_df = pd.DataFrame(test_pairs)\n",
    "\n",
    "mae = np.mean(np.abs(test_df['error']))\n",
    "rmse = np.sqrt(np.mean(np.square(test_df['error'])))\n",
    "r2 = 1 - (np.sum(np.square(test_df['error'])) / np.sum(np.square(test_df['actual_score'] - np.mean(test_df['actual_score']))))\n",
    "\n",
    "print(f\"Model Performance on Test Set:\")\n",
    "print(f\"Mean Absolute Error: {mae:.4f}\")\n",
    "print(f\"Root Mean Squared Error: {rmse:.4f}\")\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(test_df['actual_score'], test_df['predicted_score'], alpha=0.7)\n",
    "plt.plot([0, 1], [0, 1], 'r--')\n",
    "plt.xlabel('Actual Match Score')\n",
    "plt.ylabel('Predicted Match Score')\n",
    "plt.title('Model Prediction Accuracy: Actual vs Predicted Match Scores')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.text(0.05, 0.95, f\"MAE: {mae:.4f}\\nRMSE: {rmse:.4f}\\nR²: {r2:.4f}\", \n",
    "         transform=plt.gca().transAxes, bbox=dict(facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_prediction_accuracy.png')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(test_df['error'], bins=20, kde=True)\n",
    "plt.axvline(x=0, color='r', linestyle='--')\n",
    "plt.xlabel('Prediction Error (Predicted - Actual)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Prediction Errors')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('error_distribution.png')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "category_errors = test_df.groupby('resume_category')['error'].agg(['mean', 'std']).sort_values('mean')\n",
    "category_errors.plot(kind='bar', y='mean', yerr='std', capsize=4, figsize=(12, 8))\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.title('Mean Prediction Error by Resume Category')\n",
    "plt.ylabel('Mean Error')\n",
    "plt.xticks(rotation=90)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('category_error_analysis.png')\n",
    "plt.show()\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Importance': model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=feature_importance, x='Importance', y='Feature')\n",
    "plt.title('Feature Importance in Match Score Prediction')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importance.png')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTest Summary Statistics:\")\n",
    "print(test_df[['actual_score', 'predicted_score', 'error']].describe())\n",
    "\n",
    "print(\"\\nExample Job-Resume Matches:\")\n",
    "top_matches = test_df.nlargest(5, 'predicted_score')\n",
    "for i, match in top_matches.iterrows():\n",
    "    print(f\"\\nResume Category: {match['resume_category']}\")\n",
    "    print(f\"Job Title: {match['job_title']}\")\n",
    "    print(f\"Predicted Match Score: {match['predicted_score']:.4f}\")\n",
    "    print(f\"Actual Match Score: {match['actual_score']:.4f}\")\n",
    "\n",
    "    resume_skills = set(resume_df.iloc[int(match['resume_idx'])]['Skills_extracted'])\n",
    "    job_skills = set(job_df.iloc[int(match['job_idx'])]['Skills_clean'])\n",
    "    common = resume_skills.intersection(job_skills)\n",
    "    if common:\n",
    "        print(f\"Common Skills: {', '.join(list(common)[:5])}\")\n",
    "    else:\n",
    "        print(\"No common skills found\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
